{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0f82aa0917fc1d9f74e43b308994278e17c05675470959ef23ed45a89456b150f",
   "display_name": "Python 3.8.8 64-bit ('stupidconda': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('RetrievalV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['No']=data['No'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['data'] = pd.Categorical(data['data'])\n",
    "#data['data'] = data.data.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices((data['No'].astype(str).str.encode(\"utf-8\"),data['data'].astype(str).str.encode(\"utf-8\")))\n",
    "movies = tf.data.Dataset.from_tensor_slices(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfds.as_dataframe(ratings.take(10))"
   ]
  },
  {
   "source": [
    "TFRS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(2000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(1600)\n",
    "test = shuffled.skip(1600).take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(100)\n",
    "user_ids = ratings.batch(10000)\n",
    "\n",
    "unique_movie_titles = np.unique(data['data'].astype(str).str.encode(\"utf-8\"))\n",
    "unique_user_ids = np.unique(data['No'].astype(str).str.encode(\"utf-8\"))\n",
    "\n",
    "unique_movie_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[0])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[1])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoBaseClassMovielensModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Set up a gradient tape to record gradients.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Loss computation.\n",
    "      user_embeddings = self.user_model(features[0])\n",
    "      positive_movie_embeddings = self.movie_model(features[1])\n",
    "      loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "      # Handle regularization losses as well.\n",
    "      regularization_loss = sum(self.losses)\n",
    "\n",
    "      total_loss = loss + regularization_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics\n",
    "\n",
    "  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Loss computation.\n",
    "    user_embeddings = self.user_model(features[0])\n",
    "    positive_movie_embeddings = self.movie_model(features[1])\n",
    "    loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "    # Handle regularization losses as well.\n",
    "    regularization_loss = sum(self.losses)\n",
    "\n",
    "    total_loss = loss + regularization_loss\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(2000).batch(256).cache()\n",
    "cached_test = test.batch(128).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(cached_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}